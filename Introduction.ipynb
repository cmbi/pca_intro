{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PCA introduction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/niehues/6cd2916137b3838924d2874e8bce1ea4/pca-introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djCy2r7_IUeb",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to basic visualizations of multidimensional data - PCA\n",
        "\n",
        "**Topics**\n",
        "* basic data normalization\n",
        "* scatter plots, boxplots, heatmaps \n",
        "* principal component analysis (PCA)\n",
        "\n",
        "Principal component analysis (PCA) is a linear transformation method that is often used in exploratory data analysis. It is also used as a dimensionality reduction method. PCA is useful when having a data set with a large number of correlated variables. The method reduces the number of dimensions (variables) while retaining information that contributes to the variance between samples. PCA yields new dimensions called principal components that are based on the covariance or correlation of the original variables. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYwvJMVJm_Ws",
        "colab_type": "text"
      },
      "source": [
        "## Loading a data set\n",
        "\n",
        "We here use the [Breast Cancer Wisconsin (Diagnostic) Data Set](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)) [\\[1\\]](#Dua2019) as an example to illustrate different visualization methods. The data set can be loaded into Python using the [scikit-learn](https://scikit-learn.org/stable/) [\\[2\\]](#Pedregosa2011) machine learning library. Summary information on the data set is available and can be printed. The data contains 30 real-valued features (or variables) for 569 samples. The features describe characteristics of cell nuclei present in images of fine needle aspiration biopsies of breast masses. Additionally, the data set contains information on whether the breast tumors were diagnosed as malignant or benign."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8OA2MiYJT22",
        "colab_type": "code",
        "outputId": "e42b943d-d0e4-43ef-c9ff-c8fa742f8da7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn import datasets\n",
        "# load the example data set\n",
        "data = datasets.load_breast_cancer()\n",
        "# print summary information\n",
        "print(data.DESCR)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".. _breast_cancer_dataset:\n",
            "\n",
            "Breast cancer wisconsin (diagnostic) dataset\n",
            "--------------------------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 569\n",
            "\n",
            "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
            "\n",
            "    :Attribute Information:\n",
            "        - radius (mean of distances from center to points on the perimeter)\n",
            "        - texture (standard deviation of gray-scale values)\n",
            "        - perimeter\n",
            "        - area\n",
            "        - smoothness (local variation in radius lengths)\n",
            "        - compactness (perimeter^2 / area - 1.0)\n",
            "        - concavity (severity of concave portions of the contour)\n",
            "        - concave points (number of concave portions of the contour)\n",
            "        - symmetry \n",
            "        - fractal dimension (\"coastline approximation\" - 1)\n",
            "\n",
            "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
            "        largest values) of these features were computed for each image,\n",
            "        resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
            "        13 is Radius SE, field 23 is Worst Radius.\n",
            "\n",
            "        - class:\n",
            "                - WDBC-Malignant\n",
            "                - WDBC-Benign\n",
            "\n",
            "    :Summary Statistics:\n",
            "\n",
            "    ===================================== ====== ======\n",
            "                                           Min    Max\n",
            "    ===================================== ====== ======\n",
            "    radius (mean):                        6.981  28.11\n",
            "    texture (mean):                       9.71   39.28\n",
            "    perimeter (mean):                     43.79  188.5\n",
            "    area (mean):                          143.5  2501.0\n",
            "    smoothness (mean):                    0.053  0.163\n",
            "    compactness (mean):                   0.019  0.345\n",
            "    concavity (mean):                     0.0    0.427\n",
            "    concave points (mean):                0.0    0.201\n",
            "    symmetry (mean):                      0.106  0.304\n",
            "    fractal dimension (mean):             0.05   0.097\n",
            "    radius (standard error):              0.112  2.873\n",
            "    texture (standard error):             0.36   4.885\n",
            "    perimeter (standard error):           0.757  21.98\n",
            "    area (standard error):                6.802  542.2\n",
            "    smoothness (standard error):          0.002  0.031\n",
            "    compactness (standard error):         0.002  0.135\n",
            "    concavity (standard error):           0.0    0.396\n",
            "    concave points (standard error):      0.0    0.053\n",
            "    symmetry (standard error):            0.008  0.079\n",
            "    fractal dimension (standard error):   0.001  0.03\n",
            "    radius (worst):                       7.93   36.04\n",
            "    texture (worst):                      12.02  49.54\n",
            "    perimeter (worst):                    50.41  251.2\n",
            "    area (worst):                         185.2  4254.0\n",
            "    smoothness (worst):                   0.071  0.223\n",
            "    compactness (worst):                  0.027  1.058\n",
            "    concavity (worst):                    0.0    1.252\n",
            "    concave points (worst):               0.0    0.291\n",
            "    symmetry (worst):                     0.156  0.664\n",
            "    fractal dimension (worst):            0.055  0.208\n",
            "    ===================================== ====== ======\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
            "\n",
            "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
            "\n",
            "    :Donor: Nick Street\n",
            "\n",
            "    :Date: November, 1995\n",
            "\n",
            "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
            "https://goo.gl/U2Uwz2\n",
            "\n",
            "Features are computed from a digitized image of a fine needle\n",
            "aspirate (FNA) of a breast mass.  They describe\n",
            "characteristics of the cell nuclei present in the image.\n",
            "\n",
            "Separating plane described above was obtained using\n",
            "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
            "Construction Via Linear Programming.\" Proceedings of the 4th\n",
            "Midwest Artificial Intelligence and Cognitive Science Society,\n",
            "pp. 97-101, 1992], a classification method which uses linear\n",
            "programming to construct a decision tree.  Relevant features\n",
            "were selected using an exhaustive search in the space of 1-4\n",
            "features and 1-3 separating planes.\n",
            "\n",
            "The actual linear program used to obtain the separating plane\n",
            "in the 3-dimensional space is that described in:\n",
            "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
            "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
            "Optimization Methods and Software 1, 1992, 23-34].\n",
            "\n",
            "This database is also available through the UW CS ftp server:\n",
            "\n",
            "ftp ftp.cs.wisc.edu\n",
            "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
            "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
            "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
            "     San Jose, CA, 1993.\n",
            "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
            "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
            "     July-August 1995.\n",
            "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
            "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
            "     163-171.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utTRHKphLbbH",
        "colab_type": "text"
      },
      "source": [
        "In the next step, we will convert the loaded data to a [pandas](https://pandas.pydata.org/) [\\[3\\]](#Hawkins2020) DataFrame object. The data frame contains samples in rows and features in columns. This is one option to work with the tabular data and will make it easy for us to process the data and create plots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrLepkPAaK4T",
        "colab_type": "code",
        "outputId": "8807e157-9001-481b-f56c-2b1816a4707d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "import pandas as pd\n",
        "# convert data to pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns = data.feature_names)\n",
        "# print DataFrame object\n",
        "print(df)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n",
            "0          17.99         10.38  ...          0.4601                  0.11890\n",
            "1          20.57         17.77  ...          0.2750                  0.08902\n",
            "2          19.69         21.25  ...          0.3613                  0.08758\n",
            "3          11.42         20.38  ...          0.6638                  0.17300\n",
            "4          20.29         14.34  ...          0.2364                  0.07678\n",
            "..           ...           ...  ...             ...                      ...\n",
            "564        21.56         22.39  ...          0.2060                  0.07115\n",
            "565        20.13         28.25  ...          0.2572                  0.06637\n",
            "566        16.60         28.08  ...          0.2218                  0.07820\n",
            "567        20.60         29.33  ...          0.4087                  0.12400\n",
            "568         7.76         24.54  ...          0.2871                  0.07039\n",
            "\n",
            "[569 rows x 30 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcySXSHdWZa4",
        "colab_type": "code",
        "outputId": "fd38e8cf-0ea2-490b-9806-f6daa3d03bd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# print summary information for all variables\n",
        "with pd.option_context(\"display.max_columns\", df.shape[1]):\n",
        "    print(df.describe())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       mean radius  mean texture  mean perimeter    mean area  \\\n",
            "count   569.000000    569.000000      569.000000   569.000000   \n",
            "mean     14.127292     19.289649       91.969033   654.889104   \n",
            "std       3.524049      4.301036       24.298981   351.914129   \n",
            "min       6.981000      9.710000       43.790000   143.500000   \n",
            "25%      11.700000     16.170000       75.170000   420.300000   \n",
            "50%      13.370000     18.840000       86.240000   551.100000   \n",
            "75%      15.780000     21.800000      104.100000   782.700000   \n",
            "max      28.110000     39.280000      188.500000  2501.000000   \n",
            "\n",
            "       mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
            "count       569.000000        569.000000      569.000000           569.000000   \n",
            "mean          0.096360          0.104341        0.088799             0.048919   \n",
            "std           0.014064          0.052813        0.079720             0.038803   \n",
            "min           0.052630          0.019380        0.000000             0.000000   \n",
            "25%           0.086370          0.064920        0.029560             0.020310   \n",
            "50%           0.095870          0.092630        0.061540             0.033500   \n",
            "75%           0.105300          0.130400        0.130700             0.074000   \n",
            "max           0.163400          0.345400        0.426800             0.201200   \n",
            "\n",
            "       mean symmetry  mean fractal dimension  radius error  texture error  \\\n",
            "count     569.000000              569.000000    569.000000     569.000000   \n",
            "mean        0.181162                0.062798      0.405172       1.216853   \n",
            "std         0.027414                0.007060      0.277313       0.551648   \n",
            "min         0.106000                0.049960      0.111500       0.360200   \n",
            "25%         0.161900                0.057700      0.232400       0.833900   \n",
            "50%         0.179200                0.061540      0.324200       1.108000   \n",
            "75%         0.195700                0.066120      0.478900       1.474000   \n",
            "max         0.304000                0.097440      2.873000       4.885000   \n",
            "\n",
            "       perimeter error  area error  smoothness error  compactness error  \\\n",
            "count       569.000000  569.000000        569.000000         569.000000   \n",
            "mean          2.866059   40.337079          0.007041           0.025478   \n",
            "std           2.021855   45.491006          0.003003           0.017908   \n",
            "min           0.757000    6.802000          0.001713           0.002252   \n",
            "25%           1.606000   17.850000          0.005169           0.013080   \n",
            "50%           2.287000   24.530000          0.006380           0.020450   \n",
            "75%           3.357000   45.190000          0.008146           0.032450   \n",
            "max          21.980000  542.200000          0.031130           0.135400   \n",
            "\n",
            "       concavity error  concave points error  symmetry error  \\\n",
            "count       569.000000            569.000000      569.000000   \n",
            "mean          0.031894              0.011796        0.020542   \n",
            "std           0.030186              0.006170        0.008266   \n",
            "min           0.000000              0.000000        0.007882   \n",
            "25%           0.015090              0.007638        0.015160   \n",
            "50%           0.025890              0.010930        0.018730   \n",
            "75%           0.042050              0.014710        0.023480   \n",
            "max           0.396000              0.052790        0.078950   \n",
            "\n",
            "       fractal dimension error  worst radius  worst texture  worst perimeter  \\\n",
            "count               569.000000    569.000000     569.000000       569.000000   \n",
            "mean                  0.003795     16.269190      25.677223       107.261213   \n",
            "std                   0.002646      4.833242       6.146258        33.602542   \n",
            "min                   0.000895      7.930000      12.020000        50.410000   \n",
            "25%                   0.002248     13.010000      21.080000        84.110000   \n",
            "50%                   0.003187     14.970000      25.410000        97.660000   \n",
            "75%                   0.004558     18.790000      29.720000       125.400000   \n",
            "max                   0.029840     36.040000      49.540000       251.200000   \n",
            "\n",
            "        worst area  worst smoothness  worst compactness  worst concavity  \\\n",
            "count   569.000000        569.000000         569.000000       569.000000   \n",
            "mean    880.583128          0.132369           0.254265         0.272188   \n",
            "std     569.356993          0.022832           0.157336         0.208624   \n",
            "min     185.200000          0.071170           0.027290         0.000000   \n",
            "25%     515.300000          0.116600           0.147200         0.114500   \n",
            "50%     686.500000          0.131300           0.211900         0.226700   \n",
            "75%    1084.000000          0.146000           0.339100         0.382900   \n",
            "max    4254.000000          0.222600           1.058000         1.252000   \n",
            "\n",
            "       worst concave points  worst symmetry  worst fractal dimension  \n",
            "count            569.000000      569.000000               569.000000  \n",
            "mean               0.114606        0.290076                 0.083946  \n",
            "std                0.065732        0.061867                 0.018061  \n",
            "min                0.000000        0.156500                 0.055040  \n",
            "25%                0.064930        0.250400                 0.071460  \n",
            "50%                0.099930        0.282200                 0.080040  \n",
            "75%                0.161400        0.317900                 0.092080  \n",
            "max                0.291000        0.663800                 0.207500  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lCO3k1KQIin",
        "colab_type": "text"
      },
      "source": [
        "The loaded data set provides information on the breast tumor diagnosis coded as 0 and 1. We create a one-dimensional array with the diagnosis of all samples using more human readable values _malignant_ and _benign_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGbXOqzam6Tj",
        "colab_type": "code",
        "outputId": "77f8dc6f-1730-4257-dff8-af136a438a55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "# diagnosis for all samples is coded as 0 and 1 in data.target\n",
        "# the names to which 0 and 1 correspond are stored in data.target_names\n",
        "# create an array with diagnosis names of all samples\n",
        "diagnosis = pd.Series([data.target_names[_] for _ in data.target],\n",
        "                      dtype = \"category\")\n",
        "# print summary\n",
        "print(diagnosis.describe())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count        569\n",
            "unique         2\n",
            "top       benign\n",
            "freq         357\n",
            "dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGGRvnkewBUn",
        "colab_type": "text"
      },
      "source": [
        "## Data normalization (standardization)\n",
        "\n",
        "In this example data set, the units of measurements are not the same for all variables. PCA works best when the different variables are on the same scale. To achieve this, we perform a normalization procedure. The term normalization can refer to different methods. The method that we use here is called standardization. The features are standardized by **zero-centering, i.e. subtracting the mean value,** and **scaling to unit variance, i.e. dividing by the standard deviation.** The standardized values are also called z-scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV6SNAC6cj6R",
        "colab_type": "code",
        "outputId": "f114df01-47d2-4372-816b-b55b32e9812f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        }
      },
      "source": [
        "from sklearn import preprocessing\n",
        "# standardize the data (mean-centering and scaling to unit-variance)\n",
        "std_scaler = preprocessing.StandardScaler(with_mean = True, with_std = True)\n",
        "df_standardized =  pd.DataFrame(\n",
        "    std_scaler.fit_transform(df), index = df.index, columns = df.columns)\n",
        "# perform mean-centering, but no scaling\n",
        "center_scaler = preprocessing.StandardScaler(with_mean = True, with_std = False)\n",
        "df_centered =  pd.DataFrame(\n",
        "    center_scaler.fit_transform(df), index = df.index, columns = df.columns)\n",
        "# print values before and after normalization\n",
        "for _df, name in ((df, \"Raw data\"), (df_standardized)):\n",
        "    print(_df.describe())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n",
            "count   569.000000    569.000000  ...      569.000000               569.000000\n",
            "mean     14.127292     19.289649  ...        0.290076                 0.083946\n",
            "std       3.524049      4.301036  ...        0.061867                 0.018061\n",
            "min       6.981000      9.710000  ...        0.156500                 0.055040\n",
            "25%      11.700000     16.170000  ...        0.250400                 0.071460\n",
            "50%      13.370000     18.840000  ...        0.282200                 0.080040\n",
            "75%      15.780000     21.800000  ...        0.317900                 0.092080\n",
            "max      28.110000     39.280000  ...        0.663800                 0.207500\n",
            "\n",
            "[8 rows x 30 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-facc23f8a0cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     center_scaler.fit_transform(df), index = df.index, columns = df.columns)\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# print values before and after normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0m_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Raw data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf_standardized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HTC1lF1i-rM",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing the standardized data\n",
        "\n",
        "We use a boxplot and heatmap of raw and standardized data to visualize the data transformation. You can see that samples can now be compared to each other based on all variables, and not just based on those with the highest values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooszHtngbj01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# new 2 by 3 plot area\n",
        "fig, axes = plt.subplots(2, 3, figsize = (24, 16))\n",
        "for idx, _df in enumerate((df, df_centered, df_standardized)):\n",
        "    # plot boxplot\n",
        "    sns.boxplot(\n",
        "        x = \"variable\", y = \"value\", data = pd.melt(_df), ax = axes[0, idx])\n",
        "    axes[0, idx].set_xticklabels(\n",
        "        axes[0, idx].get_xticklabels(), \n",
        "        fontdict = {\"verticalalignment\": \"top\", \"horizontalalignment\": \"center\"},\n",
        "        rotation = 90)\n",
        "    # plot heatmap \n",
        "    sns.heatmap(_df, ax = axes[1, idx])\n",
        "    axes[1, idx].set_ylabel(\"samples\")\n",
        "# add plot titles\n",
        "axes[0, 0].set_title(\"Raw data\")\n",
        "axes[0, 1].set_title(\"Centered data (mean = 0)\")\n",
        "axes[0, 2].set_title(\"Standardized data (mean = 0, standard deviation = 1)\")\n",
        "# adjust horizontal padding\n",
        "plt.subplots_adjust(hspace = 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBCemfe4YLHe",
        "colab_type": "text"
      },
      "source": [
        "## Covariance and correlation (Pearson)\n",
        "\n",
        "Covariance and Pearson correlation is calculated and the correlation matrices are visualized using a heatmap. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb0JoKpZwmjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# feature correlation matrix heatmap\n",
        "fig, axes = plt.subplots(2, 2, figsize = (20, 18))\n",
        "for idx, (_df, title) in enumerate(((df, \"Raw data\"), \n",
        "                                    (df_standardized, \"Standardized data\"))):\n",
        "    # calculate covariance matrix\n",
        "    df_cov = _df.cov()\n",
        "    # calculate Pearson correlation matrix\n",
        "    df_corr = _df.corr(method = \"pearson\")\n",
        "    # plot heatmaps of matrices\n",
        "    sns.heatmap(df_cov, center = 0, cmap = \"BrBG\", ax = axes[idx, 0])\n",
        "    sns.heatmap(df_corr, center = 0, cmap = \"BrBG\", ax = axes[idx, 1])\n",
        "    # add titles\n",
        "    axes[idx, 0].set_title(\"{0} - covariance matrix\".format(title))\n",
        "    axes[idx, 1].set_title(\"{0} - correlation matrix (Pearson's r)\".format(title))\n",
        "# adjust space between plots\n",
        "plt.subplots_adjust(wspace = 0.3, hspace = 0.4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQGkYtJJTF7t",
        "colab_type": "text"
      },
      "source": [
        "In the heatmap of the correlation matrix, we can see that some features are more correlated than others. For example, `mean radius`, `mean perimeter`, and `mean area` are highly positively correlated (Pearson r ≈ 1), i.e. that there is a linear relationship between these variables (note that we haven't tested whether this correlation is statistically significant). In contrast, `mean smoothness` and `mean texture` have a low correlation coefficient (Pearson r ≈ 0). \n",
        "\n",
        "We also see that for the standardized data, the covariance matrix and the correlation matrix are identical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG_cKP_zLcGl",
        "colab_type": "text"
      },
      "source": [
        "## 2D scatter plots \n",
        "\n",
        "In this example data set, we have two categories of samples: tumors diagnosed as either malignant or benign. For each sample, we have values for 30 different features extracted from digitized biopsy images. In order to understand which of these 30 features could be suitable to distinguish a malignant from a benign sample, we can start by plotting two variables against each other.\n",
        "\n",
        "First, a function to create a scatter plot with marginal distributions (histogram and kernel density estimation) is defined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfJ1cvtQaZcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define function for scatter plot\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "def plot_joint_scatter(df, x, y, groups, group_names):\n",
        "    \"\"\"Plot joint 2D scatter plot and marginal distributions for one or more \n",
        "    groups. \n",
        "\n",
        "    Parameters:\n",
        "        df (pandas.DataFrame): Samples-by-features data frame (row x col)\n",
        "        x (str): Name of feature to plot on x-axis\n",
        "        y (str): Name of feature to plot on y-axis\n",
        "        groups (pandas.Series): Group membership of samples\n",
        "        group_names (list of str): Group names\n",
        "    \"\"\"\n",
        "    # create new plot\n",
        "    jointplot_2d = sns.JointGrid(x = x, y = y, data = df)\n",
        "    # add scatter plot\n",
        "    jointplot_2d = jointplot_2d.plot_joint(\n",
        "        sns.scatterplot, hue = groups, style = groups)\n",
        "    # add distributions (histograms)\n",
        "    for g in group_names:\n",
        "        sns.distplot(df.loc[groups == g, x], ax = jointplot_2d.ax_marg_x)\n",
        "        sns.distplot(df.loc[groups == g, y], ax = jointplot_2d.ax_marg_y, \n",
        "                    vertical = True)\n",
        "    # calculate correlation coefficient\n",
        "    r2, pval = stats.pearsonr(df[x], df[y])\n",
        "    print(\"{0}, {1}\\n\\tPearson r = {2:.2f}; p = {3:.1e}\".format(x, y, r2, pval))\n",
        "    return jointplot_2d\n",
        "\n",
        "\n",
        "def multiplot_joint_scatter(df, features_to_plot, groups, groups_names):\n",
        "    \"\"\"Combine multiple plots from 'plot_joint_scatter' into one figure.\n",
        "\n",
        "    Parameters:\n",
        "        df (pandas.DataFrame): Samples-by-features data frame (row x col)\n",
        "        features_to_plot (tuple of tuple of str): x, y feature combinations\n",
        "        groups (pandas.Series): Group membership of samples\n",
        "        group_names (list of str): Group names\n",
        "    \"\"\"\n",
        "    fig = plt.figure()\n",
        "    ax_counter = 0\n",
        "    for idx, (x, y) in enumerate(features_to_plot):\n",
        "        j = plot_joint_scatter(df, x, y, groups, groups_names)\n",
        "        for idx2, ax in enumerate(j.fig.axes):\n",
        "            # add ax to figure\n",
        "            ax.remove() # remove ax from joint plot figure 'j'\n",
        "            ax.figure = fig # add ax to new figure 'fig'\n",
        "            fig.axes.append(ax)\n",
        "            fig.add_axes(ax)\n",
        "            # shift position of each ax\n",
        "            old_pos = np.array(ax.get_position()).flatten() # x0, y0, x1, y1\n",
        "            tmp_pos = old_pos + idx # shift position \n",
        "            tmp_pos[[1, 3]] = old_pos[[1, 3]] # keep original y0 and y1\n",
        "            new_pos = [tmp_pos[0], tmp_pos[1], # x0, y0\n",
        "                    tmp_pos[2]-tmp_pos[0], tmp_pos[3]-tmp_pos[1]] # width, height\n",
        "            fig.axes[ax_counter].set_position(new_pos) # x0, y0, width, height\n",
        "            ax_counter += 1\n",
        "    return\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsqY_LC4MJ0-",
        "colab_type": "text"
      },
      "source": [
        "Next, we call the function to plot the features. Change `features_to_plot` to plot different featues against each other.\n",
        "\n",
        "It is clear that this approach is not feasible for data sets with many variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkeI3qjTm4T7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define which features to plot\n",
        "# YOU CAN CHANGE THESE VARIABLES\n",
        "features_to_plot = ((\"mean radius\", \"mean perimeter\"), \n",
        "                    (\"mean concavity\", \"mean concave points\"),\n",
        "                    (\"mean smoothness\", \"mean texture\"))\n",
        "# plot scatter plots for different combinations of given features \n",
        "multiplot_joint_scatter(df, features_to_plot, diagnosis, data.target_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIbDnpj78PW_",
        "colab_type": "text"
      },
      "source": [
        "## Principal component analysis\n",
        "\n",
        "Since, we are interested in which variables contribute most to the variance between the samples, we can apply PCA. This is an **unsupervised learning method**, i.e. that the information about the diagnosis of the samples is not used. We only use it to color the different samples. Methods that include output information like diagnosis are called supervised learning methods.\n",
        "\n",
        "### PCA with two variables\n",
        "\n",
        "To illustrate PCA, smaller data sets with two variables only are used. These are the same variables plotted above in the scatter plots.\n",
        "\n",
        "First, functions for PCA calculation and plotting are defined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21LsG1c27i8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define function for PCA with two variables only\n",
        "import numpy as np\n",
        "from sklearn import decomposition\n",
        "\n",
        "def plot_pca_two_vars(df, x, y, diagnosis, title):\n",
        "    fig, axes = plt.subplots(1, 3, figsize = (18, 5.5))\n",
        "    # scatterplot of two features\n",
        "    sns.scatterplot(\n",
        "        x = x, y = y, data = df,\n",
        "        hue = diagnosis, style = diagnosis, ax = axes[0])\n",
        "    # PCA of two features\n",
        "    pca = decomposition.PCA(n_components = 2)\n",
        "    transformed = pca.fit_transform(df.loc[:,(x,y)]) # two columns only\n",
        "    # # variance explained by principal components\n",
        "    # print(\"Explained variance:\", pca.explained_variance_ratio_)\n",
        "    # plot scores\n",
        "    sns.scatterplot(\n",
        "        x = transformed[:,0], y = transformed[:,1], # PC1, PC2\n",
        "        hue = diagnosis, style = diagnosis, ax = axes[1])\n",
        "    # add PCs to scatterplot\n",
        "    for pc, label in enumerate((\"PC 1\", \"PC 2\")):\n",
        "        axes[0].annotate(\n",
        "            \"\", \n",
        "            xy = pca.mean_, \n",
        "            xytext = pca.mean_ + 3*pca.components_[pc] * np.sqrt(pca.explained_variance_[pc]),\n",
        "            arrowprops = {\"arrowstyle\": \"<-\", \n",
        "                          \"linewidth\": 2, \n",
        "                          \"shrinkA\": 0, \n",
        "                          \"shrinkB\": 0})\n",
        "        axes[0].text(\n",
        "            *(pca.mean_ + 3.6*pca.components_[pc] * np.sqrt(pca.explained_variance_[pc])),\n",
        "            label, color = \"k\", ha = \"center\", va = \"center\")\n",
        "    # add horizontal and vertical line through origin\n",
        "    axes[1].axvline(0, c = \"k\")\n",
        "    axes[1].axhline(0, c = \"k\")\n",
        "    # normalize loadings to size of plot\n",
        "    minx, maxx = axes[1].get_xlim()\n",
        "    #print(x, y, pca.components_) # pc by var\n",
        "    loadings = pca.components_ / abs(np.amax(pca.components_)) * 0.3 * abs(maxx-minx)\n",
        "    # add loadings\n",
        "    for idx, var in enumerate((x, y)):\n",
        "        axes[1].annotate(\n",
        "            \"\", (0, 0), (loadings[0, idx], loadings[1, idx]), \n",
        "            arrowprops = {\"arrowstyle\": \"<-\", \n",
        "                          \"color\": sns.color_palette()[3],\n",
        "                          \"linewidth\": 2, \n",
        "                          \"shrinkA\": 0, \n",
        "                          \"shrinkB\": 0})\n",
        "        axes[1].text(\n",
        "            1.2*loadings[0, idx], 1.2*loadings[1, idx],\n",
        "            var, color = sns.color_palette()[3], ha = \"center\", va = \"center\")\n",
        "    # set equal dimensions for x and y axis\n",
        "    axes[0].axis(\"equal\")\n",
        "    axes[1].axis(\"equal\")\n",
        "    # plot explained variance per component and cumulative variance explained\n",
        "    expl_var = pd.DataFrame({\n",
        "        \"Principal component\": [str(pc+1) for pc in range(len(pca.explained_variance_ratio_))],\n",
        "        \"Explained variance [%]\": pd.Series(pca.explained_variance_ratio_) * 100})\n",
        "    expl_var[\"Cumulative explained variance\"] = np.cumsum(expl_var[\"Explained variance [%]\"])\n",
        "    sns.lineplot(\n",
        "        x = \"Principal component\", y = \"Cumulative explained variance\", \n",
        "        data = expl_var, ax = axes[2], color = sns.color_palette()[3], \n",
        "        label = \"Cumulative explained variance\")\n",
        "    sns.barplot(\n",
        "        x = \"Principal component\", y = \"Explained variance [%]\", \n",
        "        data = expl_var, ax = axes[2], color = sns.color_palette()[0])\n",
        "    # add labels and titles\n",
        "    axes[1].set_xlabel(\n",
        "        \"Principal component 1 ({0:.1%})\".format(pca.explained_variance_ratio_[0]))\n",
        "    axes[1].set_ylabel(\n",
        "        \"Principal component 2 ({0:.1%})\".format(pca.explained_variance_ratio_[1]))\n",
        "    axes[0].set_title(\"2D scatter plot\")\n",
        "    axes[1].set_title(\"PCA\")\n",
        "    plt.suptitle(title)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFArsHHwBhf0",
        "colab_type": "text"
      },
      "source": [
        "We now plot a scatter plot of the two variables, a PCA biplot, and a bar plot of the variance between the samples that is explained by the principal components (PCs, also called factors). This is done for both the raw data and the standardized data.\n",
        "\n",
        "The first PC is a vector that captures most of the variation between the samples. It is a linear combination of the original variables. The second PC is orthogonal to the first one.\n",
        "\n",
        "In the biplot (middle), the scores of the samples for PC 1 and PC 2 are plotted against each other. This results in a rotated version of the scatter plot on the left. \n",
        "\n",
        "The loadings of the original variables on the new factors are plotted in red (note that they are on a different scale than the scores; they are scaled to fit into the plot area). These correspond to the covariance (raw data) or correlation (standardized data) between the variables and the principal components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MplMvszT1YwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot PCA with 2 variables\n",
        "for x, y in features_to_plot:\n",
        "    plot_pca_two_vars(df, x, y, diagnosis, \n",
        "                      \"Raw data - {0}, {1}\".format(x, y))\n",
        "    plot_pca_two_vars(df_standardized, x, y, diagnosis, \n",
        "                      \"Normalized data - {0}, {1}\".format(x, y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO_wtaoS8d4i",
        "colab_type": "text"
      },
      "source": [
        "### PCA with the complete data set\n",
        "\n",
        "Finally, we perform a PCA using the complete data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "octQy-gku_wt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PCA - biplot\n",
        "from sklearn import decomposition\n",
        "fig, axes = plt.subplots(2, 2, figsize = (15, 13))\n",
        "for _df, ax, title in ((df, 0, \"Raw data\"), \n",
        "                       (df_standardized, 1, \"Normalized data\")):\n",
        "    pca = decomposition.PCA(n_components = 15)\n",
        "    transformed = pca.fit_transform(_df)\n",
        "    sns.scatterplot(\n",
        "        x = transformed[:,0], # PC1 \n",
        "        y = transformed[:,1], # PC2\n",
        "        hue = diagnosis, style = diagnosis, ax = axes[0,ax])\n",
        "    # normalize loadings to size of plot\n",
        "    minx, maxx = axes[0,ax].get_ylim()\n",
        "    loadings = pca.components_ / abs(np.amax(pca.components_[0:2,:])) * 0.4 * abs(maxx-minx)\n",
        "    # add loadings\n",
        "    for idx, var in enumerate(_df.columns):\n",
        "        axes[0,ax].text(\n",
        "            loadings[0, idx], loadings[1, idx],\n",
        "            var, color = sns.color_palette()[3], ha = \"center\", va = \"center\")\n",
        "    axes[0,ax].set_xlabel(\n",
        "        \"Principal component 1 ({0:.1%})\".format(pca.explained_variance_ratio_[0]))\n",
        "    axes[0,ax].set_ylabel(\n",
        "        \"Principal component 2 ({0:.1%})\".format(pca.explained_variance_ratio_[1]))\n",
        "    axes[0,ax].set_title(title)\n",
        "    #axes[0,ax].axis(\"equal\")\n",
        "    # plot explained variance per component and cumulative variance explained\n",
        "    expl_var = pd.DataFrame({\n",
        "        \"Principal component\": [\"{0:02d}\".format(pc+1) for pc in range(len(pca.explained_variance_ratio_))],\n",
        "        \"Explained variance [%]\": pd.Series(pca.explained_variance_ratio_) * 100})\n",
        "    expl_var[\"Cumulative explained variance\"] = np.cumsum(expl_var[\"Explained variance [%]\"])\n",
        "    sns.lineplot(\n",
        "        x = \"Principal component\", y = \"Cumulative explained variance\", \n",
        "        data = expl_var, ax = axes[1,ax], color = sns.color_palette()[3], \n",
        "        label = \"Cumulative explained variance\")\n",
        "    sns.barplot(\n",
        "        x = \"Principal component\", y = \"Explained variance [%]\", \n",
        "        data = expl_var, ax = axes[1,ax], color = sns.color_palette()[0])\n",
        "plt.suptitle(\"Principal component analysis\")\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfIrixds-HlL",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "\n",
        "1. <a name=\"Dua2019\">Dua, D. and Graff, C. (2019).</a> [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science.\n",
        "1. <a name=\"Pedregosa2011\">Pedregosa et al. (2011).</a> [Scikit-learn: Machine Learning in Python](http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html), JMLR 12, pp. 2825-2830.\n",
        "1. <a name=\"Hawkins2020\">Simon Hawkins. (2020, May 28).</a> pandas-dev/pandas: Pandas 1.0.4 (Version v1.0.4). Zenodo. http://doi.org/10.5281/zenodo.3862857"
      ]
    }
  ]
}
